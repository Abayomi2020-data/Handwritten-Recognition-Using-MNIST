{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwriting Recognition using Mnist dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Using keras on MNIST dataset for image processing\n",
        "\n",
        "### Lets start by importing the stuff we need, including the new layer types "
      ],
      "metadata": {
        "id": "OSSeL4ndX7mN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "07dTxhL8X4tr"
      },
      "outputs": [],
      "source": [
        "import tensorflow \n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading of the mnist data"
      ],
      "metadata": {
        "id": "YThvsDZUbwrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNXqsds5b8EM",
        "outputId": "13bec0e2-bbeb-4b1c-d134-08c3d3b8d04c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The next things is to shape the data differently, since we're treating the data as 2D images of 28 X 28 pixels instead of a flattened stream of 784 pixels. we need to shape it accordingly Depending on the data format keras is set up for this may be 1 x 28 x 28 or 28 x 28 x 1 the 1's indicates a single color channel as this is just grayscale if we were dealing with color images it would be 3 instead of 1 since we'd have red green and blue color channels)"
      ],
      "metadata": {
        "id": "-RfZuQUwche5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as k\n",
        "\n",
        "if k.image_data_format() == \"channels_first\":\n",
        "  train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
        "  test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28)\n",
        "  input_shape = (1, 28, 28)\n",
        "else:\n",
        "  train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
        "  test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
        "  input_shape = (28, 28, 1)\n",
        "\n",
        "  train_images = train_images.astype(\"float32\")\n",
        "  test_images = test_images.astype(\"float32\")\n",
        "  train_images /= 255\n",
        "  test_images /= 255"
      ],
      "metadata": {
        "id": "JrcKtV7bfCk_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I need to convert the train and test labels to be categorical in one-hot format"
      ],
      "metadata": {
        "id": "M1IikIRpktr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
        "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
      ],
      "metadata": {
        "id": "zYGFPbwmks1Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As a sanity check let's print out one of the training images with its label"
      ],
      "metadata": {
        "id": "8DDYFtUoiiDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def display_sample(num):\n",
        "  #print the one-hot array of this sample's label\n",
        "  print(train_labels[num])\n",
        "  #print the label converted back to a number\n",
        "  label = train_labels[num].argmax(axis=0)\n",
        "  #Reshape the 768 values to a 28X28 image\n",
        "  image = train_images[num].reshape([28, 28])\n",
        "  plt.title(\"Sample: %d Label: %d\" %(num, label))\n",
        "  plt.imshow(image, cmap=plt.get_cmap(\"gray_r\"))\n",
        "  plt.show()\n",
        "\n",
        "display_sample(1234)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "qp_pQUNtiGED",
        "outputId": "0b36955c-7caf-45e1-870c-16aca878a0ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now for the meat of the problem. setting up a convolutional neural network involues more layers. Not all of these are strickly necessary, we could run the cell without  pooling and dropout, but those extra steps help avoid overfitting and help things run faster\n",
        "### we'll start with a 2D convolution of the image-it's set up to take 32 windows, or \"filters\", of each image, each filter being 3 X 3 in size\n",
        "### we then run a second convolution on top of that with 64 3X3 windows-this topology is just what comes recommended with keras's own examples. Again we want to re-use previous research whenever possible while tuning CNN's as it is hard to do\n",
        "### Next we apply a Maxpooling2D layer that takes the maximum of each 2X2 result to distill the results down into something more manageable\n",
        "### A dropout filter is then applied to prevent overfitting\n",
        "### Next we flatten the 2D layer we have at this stage into a 1D layer. So at this point we can just pretend we have a traditional multi-layer perceptron\n",
        "### ....and feed that into a hidden, flat layer of 128units\n",
        "### We then apply dropout again to further prevent overfitting\n",
        "### And finally, we feed that into our final 10 units where softmax is apllied to choose our category of 0-9 "
      ],
      "metadata": {
        "id": "3DUNA15xnquT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3,3),\n",
        "                 activation=\"relu\",\n",
        "                 input_shape=input_shape))\n",
        "# 64 3 X 3 kernels\n",
        "model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
        "# Reduce by taking the max of each 2 X 2 block\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#Dropout to avoid overfitting\n",
        "model.add(Dropout(0.25))\n",
        "#Flatten the results to one dimention for passing into our final layer\n",
        "model.add(Flatten())\n",
        "# A hidden layer to learn with\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "# Another dropout\n",
        "model.add(Dropout(0.5))\n",
        "#Final categorization from 0-9 with softmax\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "720ql0xKtJ2D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the model description"
      ],
      "metadata": {
        "id": "yVuKvQnYxieJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAhUD0rsxw6a",
        "outputId": "2ca87717-02df-4f7b-f946-e2e29b5e1940"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1179776   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We are still doing multiple categorization, so categorical_crossentropy is still the right loss function to use. we'll use the adam optimizer, although the example provided with keras uses RMSprop. we might want to try both if we have time "
      ],
      "metadata": {
        "id": "vOi3m8t2y3oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "apQDivNByDjF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### And now we train our model... to make things go a little faster, we'll use batches of 32\n",
        "# Warning\n",
        "### This could take hours to run and computer's CPU will be maxed out during that time! Don't run the next block unless one can tie up computer for a long time. it will print progress as each epoch is run, but each epoch can take around 20 minutes \n"
      ],
      "metadata": {
        "id": "C4CffqYr1Y8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=2,\n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHdJRiSi1RqX",
        "outputId": "202d2390-75e1-42d1-9dfb-9b846e93d47d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 - 210s - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.0368 - val_accuracy: 0.9909 - 210s/epoch - 112ms/step\n",
            "Epoch 2/10\n",
            "1875/1875 - 233s - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0331 - val_accuracy: 0.9917 - 233s/epoch - 124ms/step\n",
            "Epoch 3/10\n",
            "1875/1875 - 150s - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.0363 - val_accuracy: 0.9919 - 150s/epoch - 80ms/step\n",
            "Epoch 4/10\n",
            "1875/1875 - 147s - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0365 - val_accuracy: 0.9917 - 147s/epoch - 79ms/step\n",
            "Epoch 5/10\n",
            "1875/1875 - 149s - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.0325 - val_accuracy: 0.9922 - 149s/epoch - 79ms/step\n",
            "Epoch 6/10\n",
            "1875/1875 - 148s - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.0343 - val_accuracy: 0.9932 - 148s/epoch - 79ms/step\n",
            "Epoch 7/10\n",
            "1875/1875 - 148s - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.0343 - val_accuracy: 0.9924 - 148s/epoch - 79ms/step\n",
            "Epoch 8/10\n",
            "1875/1875 - 147s - loss: 0.0160 - accuracy: 0.9952 - val_loss: 0.0388 - val_accuracy: 0.9917 - 147s/epoch - 78ms/step\n",
            "Epoch 9/10\n",
            "1875/1875 - 147s - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.0374 - val_accuracy: 0.9934 - 147s/epoch - 79ms/step\n",
            "Epoch 10/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The result is actually cool"
      ],
      "metadata": {
        "id": "9yxRhcMi6qjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(\"Test lost: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC3dl0ZGGgUx",
        "outputId": "1fda855b-3d8e-44e2-877c-d3bb051312db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test lost:  0.036077726632356644\n",
            "Test accuracy:  0.9918000102043152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Over 99% with just 10 epochs! it came at a significant cost in terms of computing power, but when we start distributing things over multiple computers each with multiple GPU's that cost starts to feel less bad. If you're building something where life and death are on the line like a self-driving car.\n",
        "### Every fraction of a percent matters\n"
      ],
      "metadata": {
        "id": "DL9hERBG7Uvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lIeFH_g48rpS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}